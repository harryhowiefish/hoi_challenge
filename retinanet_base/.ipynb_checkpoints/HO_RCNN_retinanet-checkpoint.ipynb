{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras_retinanet.models.resnet import custom_objects\n",
    "from keras_retinanet.models.resnet import resnet_retinanet as retinanet, custom_objects, download_imagenet\n",
    "# import keras_retinanet\n",
    "import keras_retinanet.bin.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_json('../train_data.json')\n",
    "train_data = train_data[['name','size','action_no','human_bbox','object_bbox','pair_no','invisible']]\n",
    "train_data = train_data.sort_values(by='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_retinanet/bin/train.py:101: UserWarning: Output \"clipped_boxes\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"clipped_boxes\" during training.\n",
      "  optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001)\n",
      "/opt/conda/lib/python3.6/site-packages/keras_retinanet/bin/train.py:101: UserWarning: Output \"nms\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"nms\" during training.\n",
      "  optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001)\n"
     ]
    }
   ],
   "source": [
    "# retinanet_model, _,_=keras_retinanet.bin.train.create_models(retinanet, 'resnet50', 80, 'resnet50_coco_best_v2.0.2.h5', \n",
    "#                                                    multi_gpu=0, freeze_backbone=True)\n",
    "retinanet_model, _,_=keras_retinanet.bin.train.create_models(retinanet, 'resnet50', 80, '/home/jovyan/keras-retinanet/snapshots/resnet50_coco_best_v2.0.2.h5', \n",
    "                                                   multi_gpu=0, freeze_backbone=True)\n",
    "# retinanet_model = keras.models.load_model('/home/jovyan/keras-retinanet/snapshots/resnet50_coco_best_v2.0.2.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_bbox(ip,threshold = 0.3):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold) & K.equal(predicted_labels,0)\n",
    "\n",
    "    scores = K.tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = K.tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = K.tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "    return [scores, boxes, classes]\n",
    "\n",
    "def obj_bbox(ip,threshold = 0.3):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold) & K.not_equal(predicted_labels,0)\n",
    "\n",
    "    scores = tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "    return [scores, boxes, classes]\n",
    "\n",
    "\n",
    "def human_stream(ip):\n",
    "    human_boxes = ip[0]\n",
    "    img_input = ip[1]\n",
    "    crop_size = tf.constant([128,128])\n",
    "    batch_inds = tf.zeros((tf.shape(human_boxes)[0],), dtype=tf.int32) \n",
    "    human_boxes_norm = human_boxes/[1200,800,1200,800]\n",
    "    human_boxes_norm = tf.stack([human_boxes_norm[:,1],human_boxes_norm[:,0],human_boxes_norm[:,3],human_boxes_norm[:,2]],axis=1)\n",
    "    result = tf.image.crop_and_resize(img_input,human_boxes_norm,batch_inds,crop_size)\n",
    "    result = (result-K.min(result))/255 \n",
    "    return [human_boxes_norm,result]\n",
    "    \n",
    "def obj_stream(ip):\n",
    "    obj_boxes = ip[0]\n",
    "    img_input = ip[1]\n",
    "    crop_size = tf.constant([128,128])\n",
    "    batch_inds = tf.zeros((tf.shape(obj_boxes)[0],), dtype=tf.int32) \n",
    "    obj_boxes_norm = obj_boxes/[1200,800,1200,800]\n",
    "    obj_boxes_norm = tf.stack([obj_boxes_norm[:,1],obj_boxes_norm[:,0],obj_boxes_norm[:,3],obj_boxes_norm[:,2]],axis=1)\n",
    "    result = tf.image.crop_and_resize(img_input,obj_boxes_norm,batch_inds,crop_size)\n",
    "    result = (result-K.min(result))/255 \n",
    "\n",
    "    return [obj_boxes_norm,result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_object_pair(ip):\n",
    "    human_boxes=ip[0]\n",
    "    obj_boxes=ip[1]\n",
    "    human_boxes_norm=ip[2]\n",
    "    obj_boxes_norm=ip[3]\n",
    "    human_count =tf.shape(human_boxes)[0]\n",
    "    obj_count = tf.shape(obj_boxes)[0]\n",
    "    ho_pair=[]\n",
    "    xx = tf.expand_dims(human_boxes, -1)\n",
    "    xx = tf.tile(xx, tf.stack([1, 1, obj_count]))\n",
    "    yy = tf.expand_dims(obj_boxes, -1)\n",
    "    yy = tf.tile(yy, tf.stack([1, 1, human_count]))\n",
    "    yy = tf.transpose(yy, perm=[2, 1, 0])       \n",
    "    ho_pair = tf.stack([xx,yy],axis=1)\n",
    "    ho_pair = tf.transpose(ho_pair,perm=[0,3,1,2])\n",
    "    ho_pair = tf.reshape(ho_pair,shape=(-1,2,4))\n",
    "    ho_pair_norm=[]\n",
    "    xx_norm = tf.expand_dims(human_boxes_norm, -1)\n",
    "    xx_norm = tf.tile(xx_norm, tf.stack([1, 1, obj_count]))\n",
    "    yy_norm = tf.expand_dims(obj_boxes_norm, -1)\n",
    "    yy_norm = tf.tile(yy_norm, tf.stack([1, 1, human_count]))\n",
    "    yy_norm = tf.transpose(yy_norm, perm=[2, 1, 0])       \n",
    "    ho_pair_norm = tf.stack([xx_norm,yy_norm],axis=1)\n",
    "    ho_pair_norm = tf.transpose(ho_pair_norm,perm=[0,3,1,2])\n",
    "    ho_pair_norm = tf.reshape(ho_pair_norm,shape=(-1,2,4))\n",
    "    return [ho_pair,ho_pair_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pattern(ho_pair):\n",
    "    pair_count = tf.shape(ho_pair)[0]\n",
    "    offset_height_h = tf.cast(ho_pair[:,0,1],tf.int32)\n",
    "    offset_width_h = tf.cast(ho_pair[:,0,0],tf.int32)\n",
    "    target_height_h = tf.cast(ho_pair[:,0,3],tf.int32) - offset_height_h \n",
    "    target_width_h = tf.cast(ho_pair[:,0,2],tf.int32) - offset_width_h\n",
    "    offset_height_o = tf.cast(ho_pair[:,1,1],tf.int32)\n",
    "    offset_width_o = tf.cast(ho_pair[:,1,0],tf.int32)\n",
    "    target_height_o = tf.cast(ho_pair[:,1,3],tf.int32) - offset_height_o\n",
    "    target_width_o = tf.cast(ho_pair[:,1,2],tf.int32) -offset_width_o\n",
    "    mask_base = tf.constant(1,shape=(800,1200,3),dtype=tf.float32)\n",
    "    i = tf.constant(0)\n",
    "    pair_mask = tf.TensorArray(dtype=tf.float32, size=pair_count)\n",
    "    def condition(i,pair_mask):\n",
    "        return i < pair_count\n",
    "    \n",
    "    def body(i,pair_mask):\n",
    "        top_bound = tf.reduce_min(tf.stack([offset_height_h[i],offset_height_o[i]]))\n",
    "        left_bound = tf.reduce_min(tf.stack([offset_width_h[i],offset_width_o[i]]))\n",
    "        bottom_bound = tf.reduce_max(tf.stack([offset_height_h[i]+target_height_h[i],offset_height_o[i]+target_height_o[i]]))\n",
    "        right_bound = tf.reduce_max(tf.stack([offset_width_h[i]+target_width_h[i],offset_width_o[i]+target_width_o[i]]))\n",
    "        mask_target_height = bottom_bound-top_bound\n",
    "        mask_target_width = right_bound-left_bound\n",
    "        mask_h = tf.image.crop_to_bounding_box(\n",
    "            mask_base,offset_height_h[i],offset_width_h[i],target_height_h[i],target_width_h[i])\n",
    "        mask_h = tf.image.pad_to_bounding_box(mask_h,offset_height_h[i]-top_bound,offset_width_h[i]-left_bound,mask_target_height,mask_target_width)\n",
    "        mask_h = tf.image.resize_image_with_crop_or_pad(mask_h,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "        mask_o = tf.image.crop_to_bounding_box(\n",
    "            mask_base,offset_height_o[i],offset_width_o[i],target_height_o[i],target_width_o[i])\n",
    "        mask_o = tf.image.pad_to_bounding_box(mask_o,offset_height_o[i]-top_bound,offset_width_o[i]-left_bound,mask_target_height,mask_target_width)\n",
    "        mask_o = tf.image.resize_image_with_crop_or_pad(mask_o,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "        mask_combine = [tf.reduce_mean(mask_h,axis=2),tf.reduce_mean(mask_o,axis=2),tf.constant(0,shape=(800,1200),dtype=tf.float32)]\n",
    "        mask_combine = tf.stack(mask_combine,axis =2)\n",
    "        mask_combine = tf.expand_dims(mask_combine,axis=0)\n",
    "        mask_combine = tf.image.resize_bilinear(mask_combine,[128,128])\n",
    "        mask_combine = tf.squeeze(mask_combine,axis=0)\n",
    "        pair_mask = pair_mask.write(i, mask_combine)\n",
    "        i = tf.add(i,1)\n",
    "        return [i, pair_mask]\n",
    "    n, pair_mask = tf.while_loop(condition, body, [i, pair_mask])\n",
    "    # get the final result\n",
    "    pair_mask_stack = pair_mask.stack()\n",
    "    return pair_mask_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = keras.layers.Input(shape=(None,None,3),name='img_input')\n",
    "\n",
    "_,_,bbox,classification=retinanet_model(img_input)\n",
    "\n",
    "human_scores, human_boxes, human_classes= keras.layers.Lambda(human_bbox)([bbox, classification])\n",
    "\n",
    "obj_scores, obj_boxes, obj_classes= keras.layers.Lambda(obj_bbox)([bbox, classification])\n",
    "\n",
    "human_boxes_norm,human_subimage = keras.layers.Lambda(human_stream)([human_boxes,img_input])\n",
    "\n",
    "obj_boxes_norm,obj_subimage = keras.layers.Lambda(obj_stream)([obj_boxes,img_input])\n",
    "\n",
    "ho_pair,ho_pair_norm = keras.layers.Lambda(human_object_pair)([human_boxes,obj_boxes,human_boxes_norm,obj_boxes_norm])\n",
    "\n",
    "pair_mask_stack = keras.layers.Lambda(attention_pattern)(ho_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = keras.Model(inputs=img_input,outputs=classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_inter = keras.Model(inputs=img_input,outputs=human_boxes)\n",
    "# test = tf.zeros((2, 180153, 80))\n",
    "# predicted_labels = K.argmax(test, axis=-1)\n",
    "# scores = K.max(test,axis=-1)\n",
    "# predicted_labels,scores\n",
    "# predicted_labels_flat()\n",
    "# predicted_labels_flat = K.flatten(predicted_labels)\n",
    "# scores_flat = K.flatten(scores)\n",
    "# filtering_mask = (scores_flat >= 0.2) & K.equal(predicted_labels_flat,0)\n",
    "# filtering_mask\n",
    "# scores_mask = K.tf.boolean_mask(scores_flat, filtering_mask) \n",
    "# # boxes = K.tf.boolean_mask(bbox, filtering_mask) \n",
    "# classes_mask = K.tf.boolean_mask(predicted_labels_flat, filtering_mask) \n",
    "# scores_mask = K.reshape(scores_mask,[2,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dim(ip):\n",
    "    human_subimage = ip[0]\n",
    "    object_subimage = ip[1]\n",
    "    pair_mask_stack = ip[2]\n",
    "    human_subimage_expand = tf.expand_dims(human_subimage,axis=0)\n",
    "    obj_subimage_expand = tf.expand_dims(obj_subimage,axis=0)\n",
    "    pair_mask_stack_expand = tf.expand_dims(pair_mask_stack,axis=0)\n",
    "    return [human_subimage_expand,obj_subimage_expand,pair_mask_stack_expand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_subimage_expand,obj_subimage_expand,pair_mask_stack_expand= keras.layers.Lambda(expand_dim)([human_subimage,obj_subimage,pair_mask_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_sum(score_600):\n",
    "    score_sum = tf.reduce_sum(score_600,axis=1)\n",
    "    return score_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#human stream\n",
    "h_conv1 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=16,kernel_size=(3,3),strides=(1, 1), padding='same'))(human_subimage_expand)\n",
    "h_pool1 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(h_conv1)\n",
    "h_conv2 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1, 1), padding='same'))(h_pool1)\n",
    "h_pool2 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(h_conv2)\n",
    "h_conv3 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1, 1), padding='same'))(h_pool2)\n",
    "h_pool3 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(h_conv3)\n",
    "h_flat = keras.layers.TimeDistributed(keras.layers.Flatten())(h_pool3)\n",
    "h_output = keras.layers.TimeDistributed(keras.layers.Dense(units=600,activation='softmax'))(h_flat)\n",
    "h_output_merge = keras.layers.Lambda(output_sum)(h_output)\n",
    "#object stream\n",
    "o_conv1 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=16,kernel_size=(3,3),strides=(1, 1), padding='same'))(obj_subimage_expand)\n",
    "o_pool1 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(o_conv1)\n",
    "o_conv2 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1, 1), padding='same'))(o_pool1)\n",
    "o_pool2 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(o_conv2)\n",
    "o_conv3 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1, 1), padding='same'))(o_pool2)\n",
    "o_pool3 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(o_conv3)\n",
    "o_flat = keras.layers.TimeDistributed(keras.layers.Flatten())(o_pool3)\n",
    "o_output = keras.layers.TimeDistributed(keras.layers.Dense(units=600,activation='softmax'))(o_flat)\n",
    "o_output_merge = keras.layers.Lambda(output_sum)(o_output)\n",
    "#pairwise stream\n",
    "p_conv1 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=16,kernel_size=(3,3),strides=(1, 1), padding='same'))(pair_mask_stack_expand)\n",
    "p_pool1 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(p_conv1)\n",
    "p_conv2 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=32,kernel_size=(3,3),strides=(1, 1), padding='same'))(p_pool1)\n",
    "p_pool2 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(p_conv2)\n",
    "p_conv3 = keras.layers.TimeDistributed(keras.layers.Conv2D(filters=64,kernel_size=(3,3),strides=(1, 1), padding='same'))(p_pool2)\n",
    "p_pool3 = keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2,2)))(p_conv3)\n",
    "p_flat = keras.layers.TimeDistributed(keras.layers.Flatten())(p_pool3)\n",
    "p_output = keras.layers.TimeDistributed(keras.layers.Dense(units=600,activation='softmax'))(p_flat)\n",
    "p_output_merge = keras.layers.Lambda(output_sum)(h_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sum = keras.layers.Add()([h_output_merge,o_output_merge,p_output_merge])\n",
    "score_sum_softmax = keras.layers.Dense(600,activation='softmax')(score_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "img_input (InputLayer)          (None, None, None, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "retinanet-bbox (Model)          [(None, None, 4), (N 38021812    img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              [(None,), (None, 4), 0           retinanet-bbox[14][2]            \n",
      "                                                                 retinanet-bbox[14][3]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              [(None,), (None, 4), 0           retinanet-bbox[14][2]            \n",
      "                                                                 retinanet-bbox[14][3]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              [(None, 4), (None, 1 0           lambda_49[0][1]                  \n",
      "                                                                 img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              [(None, 4), (None, 1 0           lambda_50[0][1]                  \n",
      "                                                                 img_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              [(None, 2, 4), (None 0           lambda_49[0][1]                  \n",
      "                                                                 lambda_50[0][1]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 128, 128, 3)  0           lambda_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              [(1, None, 128, 128, 0           lambda_51[0][1]                  \n",
      "                                                                 lambda_52[0][1]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_108 (TimeDistr (1, None, 128, 128,  448         lambda_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_116 (TimeDistr (1, None, 128, 128,  448         lambda_55[0][1]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_109 (TimeDistr (1, None, 64, 64, 16 0           time_distributed_108[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_117 (TimeDistr (1, None, 64, 64, 16 0           time_distributed_116[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_110 (TimeDistr (1, None, 64, 64, 32 4640        time_distributed_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_118 (TimeDistr (1, None, 64, 64, 32 4640        time_distributed_117[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_111 (TimeDistr (1, None, 32, 32, 32 0           time_distributed_110[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_119 (TimeDistr (1, None, 32, 32, 32 0           time_distributed_118[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_112 (TimeDistr (1, None, 32, 32, 64 18496       time_distributed_111[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_120 (TimeDistr (1, None, 32, 32, 64 18496       time_distributed_119[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_113 (TimeDistr (1, None, 16, 16, 64 0           time_distributed_112[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_121 (TimeDistr (1, None, 16, 16, 64 0           time_distributed_120[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_114 (TimeDistr (1, None, 16384)     0           time_distributed_113[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_122 (TimeDistr (1, None, 16384)     0           time_distributed_121[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_115 (TimeDistr (1, None, 600)       9831000     time_distributed_114[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_123 (TimeDistr (1, None, 600)       9831000     time_distributed_122[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (1, 600)             0           time_distributed_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (1, 600)             0           time_distributed_123[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_58 (Lambda)              (1, 600)             0           time_distributed_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (1, 600)             0           lambda_56[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "                                                                 lambda_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (1, 600)             360600      add_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 58,091,580\n",
      "Trainable params: 34,530,428\n",
      "Non-trainable params: 23,561,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_all = keras.Model(inputs=img_input,outputs=[human_boxes,obj_boxes,score_sum])\n",
    "model_all = keras.Model(inputs=img_input,outputs=score_sum_softmax)\n",
    "model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(y_true, y_pred):\n",
    "#     score_sum_true = y_true[3]\n",
    "#     score_sum_pred = y_pred[3]\n",
    "# #     human_boxes_true,obj_boxes_true,score_sum_true = y_true\n",
    "# #     human_boxes_pred,obj_boxes_pred,score_sum_pred = y_pred\n",
    "#     action_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=score_sum_true,logits=score_sum_pred)\n",
    "\n",
    "#     return action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "model_all.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_simple = pd.read_json('../train_data_simple.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(id_):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    scale_width = target_width/train_data_simple['size'][id_][0]\n",
    "    scale_height = target_height/train_data_simple['size'][id_][1]\n",
    "    human_bbox = np.array([train_data_simple['human_bbox'][id_][0]*scale_width,train_data_simple['human_bbox'][id_][1]*scale_width,\n",
    "                           train_data_simple['human_bbox'][id_][2]*scale_height,train_data_simple['human_bbox'][id_][3]*scale_height]).astype('int64')\n",
    "    object_bbox = np.array([train_data_simple['object_bbox'][id_][0]*scale_width,train_data_simple['object_bbox'][id_][1]*scale_width,\n",
    "                                       train_data_simple['object_bbox'][id_][2]*scale_height,train_data_simple['object_bbox'][id_][3]*scale_height]).astype('int64')\n",
    "    image = cv2.imread(os.path.join('/home/jovyan/projectdata/cht01/hico_20160224_det/images/train2015/',train_data_simple['name'][id_]))\n",
    "    image = cv2.resize(image,(1200,800))\n",
    "    action_array = np.zeros(600)\n",
    "    for i in train_data_simple.action_no[id_]:\n",
    "        action_array[i]=1\n",
    "    return np.expand_dims(image,axis=0),np.expand_dims(human_bbox,axis=0),np.expand_dims(object_bbox,axis=0),np.expand_dims(action_array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_image_generator(id_):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    image = cv2.imread(os.path.join('/home/jovyan/projectdata/cht01/hico_20160224_det/images/train2015/',train_data_simple['name'][id_]))\n",
    "    image = cv2.resize(image,(target_width,target_height))\n",
    "    image2 = cv2.imread(os.path.join('/home/jovyan/projectdata/cht01/hico_20160224_det/images/train2015/',train_data_simple['name'][id_+1]))\n",
    "    image2 = cv2.resize(image,(target_width,target_height))\n",
    "    return np.array([image,image2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = two_image_generator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 800, 1200, 3)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_a,y_b,y_c = train_data_generator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model_all.predict_on_batch(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 600)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c=y_c.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c.dtype,predict.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 600)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-b32ef63f45ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# predict_2 = model_all.predict(x_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    969\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining_updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0;31m# Gets loss and metrics. Updates weights at each call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvhat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvhats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mm_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mv_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    883\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m           \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# If the RHS is not a tensor, it might be a tensor aware object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "model_all.train_on_batch(x_train,y_c)\n",
    "# predict_2 = model_all.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (2, 4), (1, 600))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 4), (1, 4), (1, 600))"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_a.shape,y_b.shape,y_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_all.train_on_batch(x_train,y_c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
