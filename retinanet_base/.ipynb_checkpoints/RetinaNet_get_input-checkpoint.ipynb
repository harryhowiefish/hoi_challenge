{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images inline\n",
    "%matplotlib inline\n",
    "\n",
    "# automatically reload modules when they have changed\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from keras_retinanet.models.resnet import custom_objects\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "import keras.backend as K\n",
    "# import miscellaneous modules\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "# use this environment flag to change which GPU to use\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# set the modified tf session as backend in keras\n",
    "keras.backend.tensorflow_backend.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adjust this to point to your downloaded/trained model\n",
    "model_path = 'resnet50_coco_best_v2.0.2.h5'\n",
    "\n",
    "# load retinanet model\n",
    "retinanet = keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "#print(model.summary())\n",
    "\n",
    "# load label to names mapping for visualization purposes\n",
    "labels_to_names = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_bbox(ip,threshold = 0.6):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold) & K.equal(predicted_labels,0)\n",
    "\n",
    "    scores = tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "#     scores, boxes, classes = non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)\n",
    "    return [scores, boxes, classes]\n",
    "\n",
    "def obj_bbox(ip,threshold = 0.6):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold) & K.not_equal(predicted_labels,0)\n",
    "\n",
    "    scores = tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "#     scores, boxes, classes = non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)\n",
    "    return [scores, boxes, classes]\n",
    "\n",
    "# def non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    "\n",
    "    \n",
    "#     max_boxes_tensor = K.variable(max_boxes, dtype='int32')     # tensor to be used in tf.image.non_max_suppression()\n",
    "#     K.get_session().run(tf.variables_initializer([max_boxes_tensor])) # initialize variable max_boxes_tensor\n",
    "    \n",
    "\n",
    "#     nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold)\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     # Use K.gather() to select only nms_indices from scores, boxes and classes\n",
    "#     ### START CODE HERE ### (â‰ˆ 3 lines)\n",
    "#     scores =  K.gather(scores, nms_indices)\n",
    "#     boxes =  K.gather(boxes, nms_indices)\n",
    "#     classes =  K.gather(classes, nms_indices)\n",
    "#     ### END CODE HERE ###\n",
    "    \n",
    "#     return scores, boxes, classes\n",
    "\n",
    "def human_stream(ip):\n",
    "    human_boxes = ip[0]\n",
    "    img_input = ip[1]\n",
    "    crop_size = tf.constant([400,400])\n",
    "    batch_inds = tf.zeros((tf.shape(human_boxes)[0],), dtype=tf.int32) \n",
    "    human_boxes_norm = human_boxes/[1200,800,1200,800]\n",
    "    human_boxes_norm = tf.stack([human_boxes_norm[:,1],human_boxes_norm[:,0],human_boxes_norm[:,3],human_boxes_norm[:,2]],axis=1)\n",
    "#     img_input = tf.cast(img_input,dtype=tf.uint8)\n",
    "#     img_input = tf.image.convert_image_dtype(img_input,dtype=tf.uint8,saturate=True)\n",
    "    result = tf.image.crop_and_resize(img_input,human_boxes_norm,batch_inds,crop_size)\n",
    "#     result = tf.cast(result,dtype=tf.uint8)\n",
    "#     result = tf.image.convert_image_dtype(result,dtype=tf.uint8)\n",
    "#     result = tf.image.decode_jpeg(result)\n",
    "    result = (result-K.min(result))/255 \n",
    "    return [human_boxes_norm,result]\n",
    "    \n",
    "def obj_stream(ip):\n",
    "    obj_boxes = ip[0]\n",
    "    img_input = ip[1]\n",
    "    crop_size = tf.constant([400,400])\n",
    "    batch_inds = tf.zeros((tf.shape(obj_boxes)[0],), dtype=tf.int32) \n",
    "    obj_boxes_norm = obj_boxes/[1200,800,1200,800]\n",
    "    obj_boxes_norm = tf.stack([obj_boxes_norm[:,1],obj_boxes_norm[:,0],obj_boxes_norm[:,3],obj_boxes_norm[:,2]],axis=1)\n",
    "#     img_input = tf.cast(img_input,dtype=tf.uint8)\n",
    "#     img_input = tf.image.convert_image_dtype(img_input,dtype=tf.uint8,saturate=True)\n",
    "    result = tf.image.crop_and_resize(img_input,obj_boxes_norm,batch_inds,crop_size)\n",
    "#     result = tf.cast(result,dtype=tf.uint8)\n",
    "#     result = tf.image.convert_image_dtype(result,dtype=tf.uint8)\n",
    "#     result = tf.image.decode_jpeg(result)\n",
    "    result = (result-K.min(result))/255 \n",
    "\n",
    "    return [obj_boxes_norm,result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_object_pair(ip):\n",
    "    human_boxes=ip[0]\n",
    "    obj_boxes=ip[1]\n",
    "    human_boxes_norm=ip[2]\n",
    "    obj_boxes_norm=ip[3]\n",
    "    human_count =tf.shape(human_boxes)[0]\n",
    "    obj_count = tf.shape(obj_boxes)[0]\n",
    "    ho_pair=[]\n",
    "    xx = tf.expand_dims(human_boxes, -1)\n",
    "    xx = tf.tile(xx, tf.stack([1, 1, obj_count]))\n",
    "    yy = tf.expand_dims(obj_boxes, -1)\n",
    "    yy = tf.tile(yy, tf.stack([1, 1, human_count]))\n",
    "    yy = tf.transpose(yy, perm=[2, 1, 0])       \n",
    "    ho_pair = tf.stack([xx,yy],axis=1)\n",
    "    ho_pair = tf.transpose(ho_pair,perm=[0,3,1,2])\n",
    "    ho_pair = tf.reshape(ho_pair,shape=(-1,2,4))\n",
    "    ho_pair_norm=[]\n",
    "    xx_norm = tf.expand_dims(human_boxes_norm, -1)\n",
    "    xx_norm = tf.tile(xx_norm, tf.stack([1, 1, obj_count]))\n",
    "    yy_norm = tf.expand_dims(obj_boxes_norm, -1)\n",
    "    yy_norm = tf.tile(yy_norm, tf.stack([1, 1, human_count]))\n",
    "    yy_norm = tf.transpose(yy_norm, perm=[2, 1, 0])       \n",
    "    ho_pair_norm = tf.stack([xx_norm,yy_norm],axis=1)\n",
    "    ho_pair_norm = tf.transpose(ho_pair_norm,perm=[0,3,1,2])\n",
    "    ho_pair_norm = tf.reshape(ho_pair_norm,shape=(-1,2,4))\n",
    "    return [ho_pair,ho_pair_norm]\n",
    "#     return [xx,yy,ho_pair]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention_pattern(ho_pair):\n",
    "#     pair_count = tf.shape(ho_pair)[0]\n",
    "#     offset_height_h = tf.cast(ho_pair[:,0,1],tf.int32)\n",
    "#     offset_width_h = tf.cast(ho_pair[:,0,0],tf.int32)\n",
    "#     target_height_h = tf.cast(ho_pair[:,0,3],tf.int32) - offset_height_h \n",
    "#     target_width_h = tf.cast(ho_pair[:,0,2],tf.int32) - offset_width_h\n",
    "#     offset_height_o = tf.cast(ho_pair[:,1,1],tf.int32)\n",
    "#     offset_width_o = tf.cast(ho_pair[:,1,0],tf.int32)\n",
    "#     target_height_o = tf.cast(ho_pair[:,1,3],tf.int32) - offset_height_o\n",
    "#     target_width_o = tf.cast(ho_pair[:,1,2],tf.int32) -offset_width_o\n",
    "#     mask_base = tf.constant(1,shape=(800,1200,3))\n",
    "#     i = tf.constant(0)\n",
    "#     pair_mask = tf.TensorArray(dtype=tf.int32, size=pair_count)\n",
    "\n",
    "#     def condition(i,pair_mask):\n",
    "#         return i < pair_count\n",
    "    \n",
    "#     def body(i,pair_mask):\n",
    "#         mask_h = tf.image.crop_to_bounding_box(\n",
    "#             mask_base,offset_height_h[i],offset_width_h[i],target_height_h[i],target_width_h[i])\n",
    "#         mask_h = tf.image.resize_image_with_crop_or_pad(mask_h,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "#         mask_o = tf.image.crop_to_bounding_box(\n",
    "#             mask_base,offset_height_o[i],offset_width_o[i],target_height_o[i],target_width_o[i])\n",
    "#         mask_o = tf.image.resize_image_with_crop_or_pad(mask_o,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "#         mask_combine = [tf.reduce_mean(mask_h,axis=2),tf.reduce_mean(mask_o,axis=2),tf.constant(0,shape=(800,1200))]\n",
    "#         mask_combine = tf.stack(mask_combine,axis =2)\n",
    "#         pair_mask = pair_mask.write(i, mask_combine)\n",
    "#         i = tf.add(i,1)\n",
    "#         return [i, pair_mask]\n",
    "#     n, pair_mask = tf.while_loop(condition, body, [i, pair_mask])\n",
    "#     # get the final result\n",
    "#     pair_mask_stack = pair_mask.stack()\n",
    "\n",
    "#     return pair_mask_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_pattern(ho_pair):\n",
    "    pair_count = tf.shape(ho_pair)[0]\n",
    "    offset_height_h = tf.cast(ho_pair[:,0,1],tf.int32)\n",
    "    offset_width_h = tf.cast(ho_pair[:,0,0],tf.int32)\n",
    "    target_height_h = tf.cast(ho_pair[:,0,3],tf.int32) - offset_height_h \n",
    "    target_width_h = tf.cast(ho_pair[:,0,2],tf.int32) - offset_width_h\n",
    "    offset_height_o = tf.cast(ho_pair[:,1,1],tf.int32)\n",
    "    offset_width_o = tf.cast(ho_pair[:,1,0],tf.int32)\n",
    "    target_height_o = tf.cast(ho_pair[:,1,3],tf.int32) - offset_height_o\n",
    "    target_width_o = tf.cast(ho_pair[:,1,2],tf.int32) -offset_width_o\n",
    "    mask_base = tf.constant(1,shape=(800,1200,3))\n",
    "    i = tf.constant(0)\n",
    "    pair_mask = tf.TensorArray(dtype=tf.int32, size=pair_count)\n",
    "    def condition(i,pair_mask):\n",
    "        return i < pair_count\n",
    "    \n",
    "    def body(i,pair_mask):\n",
    "        top_bound = tf.reduce_min(tf.stack([offset_height_h[i],offset_height_o[i]]))\n",
    "        left_bound = tf.reduce_min(tf.stack([offset_width_h[i],offset_width_o[i]]))\n",
    "        bottom_bound = tf.reduce_max(tf.stack([offset_height_h[i]+target_height_h[i],offset_height_o[i]+target_height_o[i]]))\n",
    "        right_bound = tf.reduce_max(tf.stack([offset_width_h[i]+target_width_h[i],offset_width_o[i]+target_width_o[i]]))\n",
    "        mask_target_height = bottom_bound-top_bound\n",
    "        mask_target_width = right_bound-left_bound\n",
    "        mask_h = tf.image.crop_to_bounding_box(\n",
    "            mask_base,offset_height_h[i],offset_width_h[i],target_height_h[i],target_width_h[i])\n",
    "        mask_h = tf.image.pad_to_bounding_box(mask_h,offset_height_h[i]-top_bound,offset_width_h[i]-left_bound,mask_target_height,mask_target_width)\n",
    "        mask_h = tf.image.resize_image_with_crop_or_pad(mask_h,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "        mask_o = tf.image.crop_to_bounding_box(\n",
    "            mask_base,offset_height_o[i],offset_width_o[i],target_height_o[i],target_width_o[i])\n",
    "        mask_o = tf.image.pad_to_bounding_box(mask_o,offset_height_o[i]-top_bound,offset_width_o[i]-left_bound,mask_target_height,mask_target_width)\n",
    "        mask_o = tf.image.resize_image_with_crop_or_pad(mask_o,tf.shape(mask_base)[0],tf.shape(mask_base)[1])\n",
    "        mask_combine = [tf.reduce_mean(mask_h,axis=2),tf.reduce_mean(mask_o,axis=2),tf.constant(0,shape=(800,1200))]\n",
    "        mask_combine = tf.stack(mask_combine,axis =2)\n",
    "        pair_mask = pair_mask.write(i, mask_combine)\n",
    "        i = tf.add(i,1)\n",
    "        return [i, pair_mask]\n",
    "    n, pair_mask = tf.while_loop(condition, body, [i, pair_mask])\n",
    "    # get the final result\n",
    "    pair_mask_stack = pair_mask.stack()\n",
    "\n",
    "    return pair_mask_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = keras.layers.Input(shape=(None,None,3),name='img_input')\n",
    "\n",
    "_,_,bbox,classification=retinanet(img_input)\n",
    "\n",
    "human_scores, human_boxes, human_classes= keras.layers.Lambda(human_bbox)([bbox, classification])\n",
    "\n",
    "obj_scores, obj_boxes, obj_classes= keras.layers.Lambda(obj_bbox)([bbox, classification])\n",
    "\n",
    "human_boxes_norm,human_subimage = keras.layers.Lambda(human_stream)([human_boxes,img_input])\n",
    "\n",
    "obj_boxes_norm,obj_subimage = keras.layers.Lambda(obj_stream)([obj_boxes,img_input])\n",
    "\n",
    "ho_pair,ho_pair_norm = keras.layers.Lambda(human_object_pair)([human_boxes,obj_boxes,human_boxes_norm,obj_boxes_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset_height_h = tf.cast(ho_pair[:,0,1],tf.int32)\n",
    "# offset_height_o = tf.cast(ho_pair[:,1,1],tf.int32)\n",
    "# tf.stack([offset_height_h[0],offset_height_o[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_mask_stack = keras.layers.Lambda(attention_pattern)(ho_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = keras.Model(inputs=img_input,outputs=[human_scores, human_boxes, human_classes, human_boxes_norm, human_subimage,\n",
    "                                                  obj_scores, obj_boxes, obj_classes, obj_boxes_norm, obj_subimage,ho_pair,pair_mask_stack])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "image = read_image_bgr('/projectdata/cht01/hico_20160224_det/images/train2015/HICO_train2015_00000002.jpg')\n",
    "# preprocess image for network\n",
    "draw = image.copy()\n",
    "draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "\n",
    "\n",
    "\n",
    "# process image\n",
    "start = time.time()\n",
    "human_scores, human_boxes, human_classes, human_boxes_norm, human_subimage,obj_scores, obj_boxes, obj_classes, obj_boxes_norm, obj_subimage,ho_pair,pair_mask_stack= model_all.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "\n",
    "print(\"processing time: \", time.time() - start)\n",
    "\n",
    "human_boxes /= scale\n",
    "obj_boxes /= scale\n",
    "ho_pair /=scale\n",
    "\n",
    "for idx, (label, score) in enumerate(zip(human_classes, human_scores)):\n",
    "\n",
    "    color = label_color(label)\n",
    "\n",
    "    b = human_boxes[idx, :].astype(int)\n",
    "    draw_box(draw, b, color=color)\n",
    "    \n",
    "    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "    draw_caption(draw, b, caption)\n",
    "    \n",
    "for idx, (label, score) in enumerate(zip(obj_classes, obj_scores)):\n",
    "\n",
    "    color = label_color(label)\n",
    "\n",
    "    b = obj_boxes[idx, :].astype(int)\n",
    "    draw_box(draw, b, color=color)\n",
    "    \n",
    "    caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "    draw_caption(draw, b, caption)\n",
    "    \n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis('off')\n",
    "plt.imshow(draw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_mask_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pair_mask_stack[1].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obj_subimage[6][:,:,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
