{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "# from keras_retinanet.models.resnet import custom_objects\n",
    "from keras_retinanet.models.resnet import resnet_retinanet as retinanet, custom_objects, download_imagenet\n",
    "# import keras_retinanet\n",
    "import keras_retinanet.bin.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras_retinanet/bin/train.py:101: UserWarning: Output \"clipped_boxes\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"clipped_boxes\" during training.\n",
      "  optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001)\n",
      "/opt/conda/lib/python3.6/site-packages/keras_retinanet/bin/train.py:101: UserWarning: Output \"nms\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"nms\" during training.\n",
      "  optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001)\n"
     ]
    }
   ],
   "source": [
    "# retinanet_model, _,_=keras_retinanet.bin.train.create_models(retinanet, 'resnet50', 80, 'resnet50_coco_best_v2.0.2.h5', \n",
    "#                                                    multi_gpu=0, freeze_backbone=True)\n",
    "retinanet_model, _,_=keras_retinanet.bin.train.create_models(retinanet, 'resnet50', 80, '/home/jovyan/keras-retinanet/snapshots/resnet50_coco_best_v2.0.2.h5', \n",
    "                                                   multi_gpu=0, freeze_backbone=True)\n",
    "# retinanet_model = keras.models.load_model('/home/jovyan/keras-retinanet/snapshots/resnet50_coco_best_v2.0.2.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_bbox(ip,threshold = 0.2):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold) & K.equal(predicted_labels,0)\n",
    "\n",
    "    scores = tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "#     scores, boxes, classes = non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)\n",
    "    return [scores, boxes, classes]\n",
    "\n",
    "def obj_bbox(ip,threshold = 0.2):\n",
    "    bbox = ip[0]\n",
    "    classification = ip[1]\n",
    "\n",
    "    predicted_labels = K.argmax(classification, axis=2)\n",
    "    scores = K.max(classification,axis=2)\n",
    "\n",
    "    filtering_mask = (scores >= threshold)\n",
    "\n",
    "    scores = tf.boolean_mask(scores, filtering_mask) \n",
    "    boxes = tf.boolean_mask(bbox, filtering_mask) \n",
    "    classes = tf.boolean_mask(predicted_labels, filtering_mask) \n",
    "#     scores, boxes, classes = non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)\n",
    "    return [scores, boxes, classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = keras.layers.Input(shape=(None,None,3),name='img_input')\n",
    "\n",
    "_,_,bbox,classification=retinanet_model(img_input)\n",
    "\n",
    "human_scores, human_boxes, human_classes= keras.layers.Lambda(human_bbox)([bbox, classification])\n",
    "\n",
    "obj_scores, obj_boxes, obj_classes= keras.layers.Lambda(obj_bbox)([bbox, classification])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = keras.Model(inputs=img_input,outputs=[obj_scores, obj_boxes, obj_classes,\n",
    "                                                 human_scores, human_boxes, human_classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_simple = pd.read_json('../train_data_simple.json')\n",
    "train_data_simple = train_data_simple.sort_index()\n",
    "test_data = pd.read_json('../test_data.json')\n",
    "test_data = test_data.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(id_):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    scale_width = target_width/train_data_simple['size'][id_][0]\n",
    "    scale_height = target_height/train_data_simple['size'][id_][1]\n",
    "    image = cv2.imread(os.path.join('/home/jovyan/projectdata/cht01/hico_20160224_det/images/train2015/',train_data_simple['name'][id_]))\n",
    "    image = cv2.resize(image,(target_width,target_height))\n",
    "    return np.expand_dims(image,axis=0)\n",
    "def test_data_generator(id_):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    scale_width = target_width/test_data['size'][id_][0]\n",
    "    scale_height = target_height/test_data['size'][id_][1]\n",
    "    image = cv2.imread(os.path.join('/home/jovyan/projectdata/cht01/hico_20160224_det/images/test2015/',test_data['name'][id_]))\n",
    "    image = cv2.resize(image,(target_width,target_height))\n",
    "    return np.expand_dims(image,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_obj_scores, pred_obj_boxes, pred_obj_classes,pred_human_scores, pred_human_boxes, pred_human_classes =[],[],[],[],[],[]\n",
    "for i in tqdm(range(len(train_data_simple))):\n",
    "    img = train_data_generator(i)\n",
    "    a,b,c,d,e,f = model_all.predict_on_batch(img)\n",
    "    pred_obj_scores.append(a)\n",
    "    pred_obj_boxes.append(b)\n",
    "    pred_obj_classes.append(c)\n",
    "    pred_human_scores.append(d)\n",
    "    pred_human_boxes.append(e)\n",
    "    pred_human_classes.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a945488bad224a17a779e338b530c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9658), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_obj_scores, test_obj_boxes, test_obj_classes,test_human_scores, test_human_boxes, test_human_classes =[],[],[],[],[],[]\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    img = test_data_generator(i)\n",
    "    a,b,c,d,e,f = model_all.predict_on_batch(img)\n",
    "    test_obj_scores.append(a)\n",
    "    test_obj_boxes.append(b)\n",
    "    test_obj_classes.append(c)\n",
    "    test_human_scores.append(d)\n",
    "    test_human_boxes.append(e)\n",
    "    test_human_classes.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_train_data = pd.DataFrame({\n",
    "    'obj_scores':pred_obj_scores,\n",
    "    'obj_boxes':pred_obj_boxes,\n",
    "    'obj_classes':pred_obj_classes,\n",
    "    'human_scores':pred_human_scores, \n",
    "    'human_boxes':pred_human_boxes, \n",
    "    'human_classes':pred_human_classes\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_test_data = pd.DataFrame({\n",
    "    'obj_scores':test_obj_scores,\n",
    "    'obj_boxes':test_obj_boxes,\n",
    "    'obj_classes':test_obj_classes,\n",
    "    'human_scores':test_human_scores, \n",
    "    'human_boxes':test_human_boxes, \n",
    "    'human_classes':test_human_classes\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_train_data.human_boxes = [np.array(item) for item in retinanet_train_data.human_boxes.tolist()]\n",
    "retinanet_train_data.obj_boxes = [np.array(item) for item in retinanet_train_data.obj_boxes.tolist()]\n",
    "retinanet_test_data.human_boxes = [np.array(item) for item in retinanet_test_data.human_boxes.tolist()]\n",
    "retinanet_test_data.obj_boxes = [np.array(item) for item in retinanet_test_data.obj_boxes.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x1, y1, x2, y2)\n",
    "human_boxes_scale = []\n",
    "obj_boxes_scale = []\n",
    "for id_ in tqdm(range(len(retinanet_train_data))):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    scale_width = target_width/train_data_simple['size'][id_][0]\n",
    "    scale_height = target_height/train_data_simple['size'][id_][1]\n",
    "    if retinanet_train_data['human_boxes'][id_].any():\n",
    "        human_bbox = np.array([retinanet_train_data['human_boxes'][id_][:,0]/scale_width,retinanet_train_data['human_boxes'][id_][:,1]/scale_height,\n",
    "                               retinanet_train_data['human_boxes'][id_][:,2]/scale_width,retinanet_train_data['human_boxes'][id_][:,3]/scale_height]).astype('int64')\n",
    "    else:\n",
    "        human_bbox = np.array([])\n",
    "    if retinanet_train_data['obj_boxes'][id_].any():\n",
    "        object_bbox = np.array([retinanet_train_data['obj_boxes'][id_][:,0]/scale_width,retinanet_train_data['obj_boxes'][id_][:,1]/scale_height,\n",
    "                            retinanet_train_data['obj_boxes'][id_][:,2]/scale_width,retinanet_train_data['obj_boxes'][id_][:,3]/scale_height]).astype('int64')\n",
    "    else:\n",
    "        object_bbox = np.array([])\n",
    "    human_bbox = np.transpose(human_bbox)\n",
    "    object_bbox = np.transpose(object_bbox)\n",
    "    human_boxes_scale.append(human_bbox)\n",
    "    obj_boxes_scale.append(object_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_train_data['human_boxes_scale'] = human_boxes_scale\n",
    "retinanet_train_data['obj_boxes_scale'] = obj_boxes_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037cbe3ad0f3471aaff83f1b8ac30e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9658), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# (x1, y1, x2, y2)\n",
    "human_boxes_scale = []\n",
    "obj_boxes_scale = []\n",
    "for id_ in tqdm(range(len(retinanet_test_data))):\n",
    "    target_width = 1200\n",
    "    target_height = 800\n",
    "    scale_width = target_width/test_data['size'][id_][0]\n",
    "    scale_height = target_height/test_data['size'][id_][1]\n",
    "    if retinanet_test_data['human_boxes'][id_].any():\n",
    "        human_bbox = np.array([retinanet_test_data['human_boxes'][id_][:,0]/scale_width,retinanet_test_data['human_boxes'][id_][:,1]/scale_height,\n",
    "                               retinanet_test_data['human_boxes'][id_][:,2]/scale_width,retinanet_test_data['human_boxes'][id_][:,3]/scale_height]).astype('int64')\n",
    "    else:\n",
    "        human_bbox = np.array([])\n",
    "    if retinanet_test_data['obj_boxes'][id_].any():\n",
    "        object_bbox = np.array([retinanet_test_data['obj_boxes'][id_][:,0]/scale_width,retinanet_test_data['obj_boxes'][id_][:,1]/scale_height,\n",
    "                            retinanet_test_data['obj_boxes'][id_][:,2]/scale_width,retinanet_test_data['obj_boxes'][id_][:,3]/scale_height]).astype('int64')\n",
    "    else:\n",
    "        object_bbox = np.array([])\n",
    "    human_bbox = np.transpose(human_bbox)\n",
    "    object_bbox = np.transpose(object_bbox)\n",
    "    human_boxes_scale.append(human_bbox)\n",
    "    obj_boxes_scale.append(object_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_test_data['human_boxes_scale'] = human_boxes_scale\n",
    "retinanet_test_data['obj_boxes_scale'] = obj_boxes_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet_train_data.to_json('retinanet_train_data.json')\n",
    "retinanet_test_data.to_json('retinanet_test_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_no = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_no=35368\n",
    "image_no+=1\n",
    "print(image_no)\n",
    "input_img = cv2.imread('/home/jovyan/projectdata/cht01/hico_20160224_det/images/train2015/'+train_data_simple['name'][image_no])\n",
    "RGB_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "# RGB_img = cv2.resize(RGB_img,(1200,800))\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(16,8)\n",
    "for i in retinanet_train_data['human_boxes_scale'][image_no]:\n",
    "    cv2.rectangle(RGB_img, (i[0],i[1]),(i[2],i[3]), (255,0,255), 2)\n",
    "ax1.imshow(RGB_img)\n",
    "for i in retinanet_train_data['obj_boxes_scale'][image_no]:\n",
    "    cv2.rectangle(RGB_img, (i[0],i[1]),(i[2],i[3]), (0,255,0), 2)\n",
    "ax2.imshow(RGB_img)\n",
    "\n",
    "# for i in train_data_simple['action_no'][image_no]:\n",
    "#     print(hoi_dict[str(i)])\n",
    "print(train_data_simple.iloc[image_no])\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.imshow(RGB_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
